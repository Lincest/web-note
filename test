Published as a conference paper at ICLR 2023
A2Q: AGGREGATION-AWARE QUANTIZATION FOR GRAPH NEURAL NETWORKS
Zeyu Zhu1,2 Fanrong Li2 Zitao Mo2 Qinghao Hu2 Gang Li3 Zejian Liu2 Xiaoyao Liang3 Jian Cheng2∗
1School of Future Technology, University of Chinese Academy of Sciences
2Institute of Automation, Chinese Academy of Sciences
3Shanghai Jiao Tong University
{zhuzeyu2021, lifanrong2017, mozitao2017}@ia.ac.cn, {huqinghao2014, liuzejian2018}@ia.ac.cn, {gliaca}@sjtu.edu.cn {liang-xy}@cs.sjtu.edu.cn
{jcheng}@nlpr.ia.ac.cn
ABSTRACT
As graph data size increases, the vast latency and memory consumption during in- ference pose a significant challenge to the real-world deployment of Graph Neural Networks (GNNs). While quantization is a powerful approach to reducing GNNs complexity, most previous works on GNNs quantization fail to exploit the unique characteristics of GNNs, suffering from severe accuracy degradation. Through an in-depth analysis of the topology of GNNs, we observe that the topology of the graph leads to significant differences between nodes, and most of the nodes in a graph appear to have a small aggregation value. Motivated by this, in this paper, we propose the Aggregation-Aware mixed-precision Quantization (A2Q) for GNNs, where an appropriate bitwidth is automatically learned and assigned to each node in the graph. To mitigate the vanishing gradient problem caused by sparse connections between nodes, we propose a Local Gradient method to serve the quantization error of the node features as the supervision during training. We also develop a Nearest Neighbor Strategy to deal with the generalization on unseen graphs. Extensive experiments on eight public node-level and graph-level datasets demonstrate the generality and robustness of our proposed method. Compared to the FP32 models, our method can achieve up to a 18.6x (i.e., 1.70bit) compression ratio with negligible accuracy degradation. Morever, compared to the state-of-the- art quantization method, our method can achieve up to 11.4% and 9.5% accuracy improvements on the node-level and graph-level tasks, respectively, and up to 2x speedup on a dedicated hardware accelerator.
1 INTRODUCTION
Recently, Graph Neural Networks (GNNs) have attracted much attention due to their superior learn- ing and representing ability for non-Euclidean geometric data. A number of GNNs have been widely used in real-world applications, such as recommendation system (Jin et al., 2020), and social net- work analysis (Lerer et al., 2019), etc. Many of these tasks put forward high requirements for low- latency inference. However, the real-world graphs are often extremely large and irregular, such as Reddit with 232,965 nodes, which needs 19G floating-point operations (FLOPs) to be processed by a 2-layer Graph Convolutional Network (GCN) with only 81KB parameters (Tailor et al., 2020), while ResNet-50, a 50-layer DNN, only takes 8G FLOPs to process an image (Canziani et al., 2016). What is worse, it requires a huge amount of memory access for GNNs inference, e.g., the nodes features size of Reddit is up to 534MB, leading to high latency. Therefore, the aforementioned problems pose a challenge to realize efficient inference of GNNs.
Neural network quantization can reduce the model size and accelerate inference without modify- ing the model architecture, which has become a promising method to solve this problem in re-
∗Corresponding author
1
 arXiv:2302.00193v1 [cs.LG] 1 Feb 2023
 
Published as a conference paper at ICLR 2023
   




@ @
@ @ QGHUHH
@
$YHUDH)HDWXUH$IWHU$UDWRQ
 &1
 1 $7
      





@ @
@ QGHUHH
@
@
   $YHUDH)HDWXUH$IWHU$UDWRQ
1BDHU 1BDHU
 1BDHU 1BDHU
                      (a) (b)
Figure 1: The analysis of the average aggerated node features in different in-degrees node groups on various tasks. (a) The values at the final layer for GNNs trained on Cora. (b) The values at the 2-5 layer of GIN trained on REDDIT-BINARY. The average values are all generated from 10 runs.
cent years. Unfortunately, there remain some issues in the existing works on GNNs quantization. Feng et al. (2020) only quantizes the node feature and keeps floating point calculations during infer- ence. Tailor et al. (2020) proposes a degree-quant training strategy to quantize GNNs to the low-bit fixed point but causes a large accuracy drop, e.g., 11.1% accuracy drops when quantizing to 4bits. Moreover, some works (Wang et al., 2021b; Bahri et al., 2021; Wang et al., 2021a; Jing et al., 2021) quantize GNNs into 1-bit and compute with XNOR and bit count operations. However, these 1-bit quantization methods are either restricted to the node-level tasks or can not generalize well to other GNNs.
Most of the above methods do not make full use of the property of GNNs and graph data, re- sulting in severe accuracy degradation or poor generalization. As presented in MPNN framework (Gilmer et al., 2017), GNNs processing is divided into two phase: First, in the aggregation phase, a node collects information from neighboring nodes and uses the aggregation function to generate hidden features; second, in the update phase, the hidden features are transformed into new features by an update function. We analyze the nodes features after aggregation in Figure 1 and find that the higher the in-degree is, the larger the node features tend to be after aggregation. And the fea- tures vary significantly between nodes with different in-degrees, which represent the topology of a graph. Moreover, according to Xie et al. (2014); Aiello et al. (2001), the degrees of nodes in most real-world graph data often follow the power-law distribution, i.e., nodes with a low degree account for the majority of graph data. Therefore, specially quantizing the nodes features according to the topology of the graphs will be beneficial to reduce the quantization error while achieving a higher compression ratio.
In this paper, we propose the Aggregation-Aware Quantization (A2Q) method, which quantizes different nodes features with different learnable quantization parameters, including bitwidth and step size. These parameters can be adaptively learned during training and are constrained by a penalty on memory size to improve the compression ratio. However, when quantizing the model in semi-supervised tasks, the gradients for most quantization parameters are zero due to the sparse connections between nodes, which makes the training non-trivial. We propose the Local Gradient method to solve this problem by introducing quantization error as supervised information. Finally, to generalize our method to unseen graphs in which the number of the nodes varies, we develop the Nearest Neighbor Strategy which assigns the learned quantization parameters to the unseen graph nodes. To the best of our knowledge, we are the first to introduce the mixed-precision quantization to the GNNs. Compared with the previous works, our proposed methods can significantly compress GNNs with negligible accuracy drop.
In summary, the key contributions of this paper are as follows:
1) WeproposetheAggregation-Awaremixed-precisionQuantization(A2Q)methodtoenable an adaptive learning of quantization parameters. Our learning method is powerful by fully
2

Published as a conference paper at ICLR 2023
 utilizing the characteristic of GNNs, and the learned bitwidth is strongly related to the topology of the graph.
2) A Local Gradient method is proposed to train the quantization parameters in semi- supervised learning tasks. Furthermore, to generalize our method to the unseen graphs in which the number of input nodes is variable, we develop the Nearest Neighbor Strategy to select quantization parameters for the nodes of the unseen graphs.
3) Experiments demonstrate that we can achieve a compression ratio up to 18.6x with negli- gible accuracy degradation compared to the full-precision (FP32) models. Moreover, the model trained with our A2Q method outperforms the state-of-the-art (SOTA) method up to 11.4% with a speedup up to 2.00x in semi-supervised tasks, and obtains up to 9.5% gains with a 1.16x speedup in graph-level tasks. We provide our code at this URL: https://github.com/weihai-98/A2Q.
2 RELATED WORK
Graph Neural Networks: The concept of the graph neural network was first proposed in Scarselli et al. (2008), which attempted to generalize neural networks to model non-Euclidean data. In the following years, various GNN models were proposed. For example, Graph Convolution Net- work (GCN) (Kipf & Welling, 2016) uses a layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs, Graph Isomorphism Network (GIN) (Xu et al., 2018) designed a provably maximally powerful GNN under the MPNN framework, and Graph At- tention Network (GAT) (Velicˇkovic ́ et al., 2017) introduces the attention mechanism to graph pro- cessing. Although GNNs have encouraging performance in a wide range of domains (Jin et al., 2020; Yang, 2019), the huge amount of float-point operations and memory access in process pose a challenge to efficient inference, which hinder the applications of GNNs.
Quantized GNNs: As a promising method to reduce the model size and accelerate the inference process, quantization is also applied to GNNs. Some works quantize features and weights in GNNs to low bitwidths (Feng et al., 2020; Tailor et al., 2020) or even 1-bit (Wang et al., 2021b; Bahri et al., 2021; Wang et al., 2021a; Jing et al., 2021), i.e., use fixed-point numbers instead of floating-point numbers for computation. But when the compression ratio is high (e.g., <4bit), the performance degradation of these works is significant, and the generalization of 1-bit method is limited. There are also some works on vector quantization (VQ), which use the vectors in a codebook obtained during the training process instead of the original features (Ding et al., 2021; Huang et al., 2022). However, searching for vectors in the codebook is computationally complex.
Mixed-Precision Quantization: Based on the idea that different layers have different sensitiv- ities to quantization, mixed-precision quantization is proposed in CNNs to quantize different layers to different bitwidths for better model compression. Early works (Wang et al., 2019; Lou et al., 2019) proposed reinforcement learning (RL) based methods to search bitwidth for different lay- ers, but they often require large computational resources, which limits the exploration of the search space. Another important class of mixed-precision method is the criteria-based method, they use the specific criteria to represent the quantization sensitivity, e.g., (Dong et al., 2019; 2020; Chen et al., 2021)quantize different layers with different bitwidths based on the trace of the Hessian. Recently, there are some other methods to learn the bitwidth during training (Uhlich et al., 2019; Esser et al., 2019; Jain et al., 2020). However, due to the huge difference between GNNs and CNNs, it is dif- ficult to use these methods on GNNs directly, and our A2Q is the first method to introduce the mixed-precision quantization to GNNs, further improving the inference efficiency of GNNs.
3 METHOD
In this section, we describe our proposed Aggregation-Aware Quantization in detail. Firstly, we present the formulation of the mixed-precision quantization for GNNs, which fully utilizes the prop- erty of GNNs and graph data. Secondly, we introduce the Local Gradient method to address the gradient vanishing problem during training. Finally, we detail the Nearest Neighbor Strategy, which is used for generalizing our approach to the unseen graphs.
3

Published as a conference paper at ICLR 2023
          !""
!"#
!"$
!#"
!##
!#$
!$"
!$#
!$$
!%"
!%#
!%$
!&"
!&#
!&$
!'"
!'#
!'$
 ,""
,"#
,#"
,##
,$"
,$#
,%"
,%#
,&"
,&#
,'"
,'#
 ("
(# N ($ (% (& ('
F
Nodes Features
F2
+" !
¥=
Weights
(" -"
(# -"
 *)""
*)"#
*)#"
*)##
*)$"
*)$#
($ -"
 (% -"
(&-"
(' -"
(" -#
(# -#
($ -#
(% -#
(& -#
(' -#
 Figure 2: Perform matrix multiplication by the Figure 3: The gradients to xq in GCN trained on integer represented. x ̄ and w ̄ are both integers. Cora by sampling 400 nodes.
3.1 AGGREGATION-AWARE QUANTIZATION
We assume a graph data with N nodes and the node features are F -dimensional, i.e., the feature map is X ∈ RN ×F and xi is the features of node i. We use the learnable parameters step size αi ∈ R+ and bitwidth bi ∈ R+ to quantize the features of the i-th node as:
⌊|xi|+0.5⌋, |x|<α(2[b]−1−1) αi ii
 2[bi]−1 − 1, |xi| ≥ αi(2[bi]−1 − 1)
where ⌊·⌋ is the floor function, and [·] is the round function to ensure the bitwidth used to quantize is an integer. The learnable parameters are sX = (α1, α2, ..., αN ), and bX = (b1, b2, ..., bN ). Then we can obtain the fixed-point feature map X ̄ , and the original feature can be represented as Xq = SX · X ̄ , where SX = diag(α1, α2, ..., αN ). Note that we use [b] + 1 as the quantization bitwidth for the features after ReLU because the values are all non-negative.
In the update phase, the node features are often transformed with a linear mapping or an MLP in which matrix multiplication XW is the main computation, and the transformed node features are the input to the next layer in GNNs. In order to accelerate the update phase, we also quantize W . Due to the fact that W in a certain layer is shared by all nodes, we quantize W to the same bitwidth of 4bits for all GNNs in this paper. However, each column of W has its learnable quantization step size, i.e., sW = (β1, β2, .., βF2 ), where F2 is the output-dimension of the node features in current layer and βi is the quantization step size for the i-th column of W , and we also use Eq. 1 to quantize W . We can obtain the integer representation W ̄ and the quantized representation Wq = W ̄ · SW , where SW = diag(β1, β2, ..., βF2 ). The float-point matrix multiplication in the update phase can be reformulated as follow:
X · W ≈ X q · W q = ( S X · X ̄ ) · ( W ̄ · S W ) = ( X ̄ · W ̄ ) ⊙ ( s X ⊗ s W ) , ( 2 )
where ⊙ denotes an element-wise multiplication, and ⊗ denotes the outer product. After training, we can obtain sX and sW so that the outer product can be pre-processed before inference. An example is illustrated in Figure 2. For the aggregation phase, i.e., AX, A is the adjacency matrix and A ∈ {0, 1}N ×N , we quantize the X as the quantization way of W because the nodes features involved in the aggregation process come from the update phase, in which the features lose the topology information of graphs. Then the aggregation phase can be performed by integer operations to reduce the computational overhead.
The quantization parameters (s, b) are trained by the backpropagation algorithm. Since the floor and round functions used in the quantization process are not differentiable, we use the straight- through estimator (Bengio et al., 2013) to approximate the gradient through these functions, and the gradients of the quantization parameters can be calculated by:
∂L=Xd ∂L·∂xiq, (3) ∂L=Xd ∂L·∂xiq, (4) ∂s i=1 ∂xiq ∂s ∂b i=1 ∂xiq ∂b
where d is the dimension of the vector x, (s, b) are the quantization parameters for x, and xiq is the value of i-th dimension in xq. Detailed information about quantization process and the backpropagation are shown in Appendix A.1 and A.3 Proof 2 and 3.
 x ̄i = sign(xi)
, (1)
      4

Published as a conference paper at ICLR 2023
 In order to improve the compression ratio of the node features, we introduce a penalty term on the memory size:
1LN
Lmemory = (η ·XXdiml ·bli −Mtarget)2 , (5)
l=1 i=1
where L is the number of layers in the GNNs, N is the total number of nodes, diml is the length of the node features in l-th layer, bli is the quantization bitwidth for node i in l-th layer, Mtarget is the target memory size on the total node features memory size, and η = 8 ∗ 1024, which is a constant to convert the unit of memory size to KB. Then the model and quantization parameters can be trained by the loss function:
Ltotal = Ltask + λ · Lmemory , (6) where Ltask is the task-related loss function and λ is a penalty factor on Lmemory.
3.2 LOCAL GRADIENT
Although the above end-to-end learning method is concise and straightforward, the gradients for
the quantization parameters of nodes features, i.e., ∂Ltask and ∂Ltask , are almost zero during the ∂s ∂b
training process of semi-supervised tasks, which poses a significant challenge to train the quantiza-
tion parameters for nodes features. We analyze the property of GNNs and graph data, and find that
two reasons lead to this phenomenon: 1. The extreme sparsity of the connections between nodes
in graph data. 2. Only a tiny fraction of nodes with labels are used for training in semi-supervised
tasks (e.g., 0.30% in PubMed dataset). Therefore, ∂Ltask for most node features are zero (detailed ∂xq
    proof in Appendix A.3.2), which results in that the gradients for quantization parameters of these nodes vanish according to Eq. 3 and Eq. 4. To clarify, we visualize the ∂Ltask in the second layer
 ∂xq
of GCN trained on Cora. As shown in Figure 3, most gradients for the nodes features are zero.
The gradients of the Ltask w.r.t. quantized nodes features can be viewed as the supervised infor-
mation from the labeled nodes which enable the training of the quantization parameters for nodes
features. However, this supervised information is missing due to zero gradients. Considering the
quantization error is related to the Ltask , we introduce the quantization error E = 1 |xq − x| as d1
the supervised information for the quantization parameters of nodes features, where x is the features before quantization, xq is the features after quantization and | · |1 denotes the L1 norm. We refer to this method as Local Gradient because the gradients are computed by the local quantization er- rors instead of back-propagated task-related gradients. Then the quantization parameters for node features can be trained by gradients from E:
 ∂E 1d ∂xi
= Xsign(xiq −xi)· q , (7)
∂E 1d ∂xi
= Xsign(xiq −xi)· q . (8)
      ∂s d i=1 ∂s
Note that the quantization parameters of W are still trained by utilizing the gradients in Eq. 3.
3.3 NEAREST NEIGHBOR STRATEGY
In graph-level tasks, the quantized GNNs are required to generalize to unseen graphs. In such a scenario, the number of input nodes may vary during training or inference. However, the learnable method can only train a fixed number of (s, b) pairs which are the same as the number of input nodes, so it is challenging to learn the s and b for every node in graph-level tasks. To solve this problem, we propose the Nearest Neighbor Strategy, which allows learning of a fixed number of quantization parameters and select quantization parameters for the unseen graphs.
The proposed strategy is shown in Algorithm 1. To ensure the numerical range of xq is as close as to x at FP32, a simple way is to keep the maximum quantization value equal to the maximum absolute value of x. Based on this idea, we first initialize m groups of quantization parameters, then we calculate the maximum quantization value for every group, i.e., qmax = s(2[b]−1 − 1). When quantizing the features of node i, the feature with the largest absolute value fi in the node features xi is first selected, and then we find the nearest qmax and quantize the node features with the (s, b) corresponding to this qmax. When performing backpropagation, we first calculate the gradients of the loss function w.r.t. quantization parameters according to Eq. 3 and Eq. 4. For a specific set of quantization parameters (sj,bj), we collect the gradients from the nodes that have used them
5
∂b d i=1 ∂b

Published as a conference paper at ICLR 2023
  Algorithm 1 Nearest Neighbor Strategy
1: ForwardPass (X = (x1, x2, ..., xN )T ):
2: Initialize(s, b), s ∈ Rm×1, b ∈ Rm×1 before training ++
3: Calculate qmax = s ⊙ (2b−1 − 1)
4: Calculate the maximum absolute value in the features of each node: fi = max abs(x(j))
 5: Search the index of quantization parameters for each node: indexi = arg min |fi − qmk ax | k
6: Quantize the i-th node features using (sindexi , bindexi )
7: return Xq
8: end
Table 1: The results comparison on node-level tasks. The average bits are counted for each task when the best results are achieved.
ji
   Dataset
Cora
CiteSeer
PubMed ogbn-arxiv
Model
GCN(FP32) GCN(DQ ) GCN(ours) GAT(FP32) GAT(DQ ) GAT(ours) GCN(FP32) GCN(DQ ) GCN(ours) GIN(FP32) GIN(DQ ) GIN(ours) GAT(FP32) GAT(DQ) GAT(ours) GCN(FP32) GCN(DQ) GCN(ours)
Accuracy
81.5±0.7% 78.3±1.7% 80.9±0.6% 83.1±0.4% 71.2±2.9% 82.6±0.6% 71.1±0.7% 66.9±2.4% 70.6±1.1% 66.1±0.9% 60.8±2.1% 65.1±1.7% 79.0±0.3% 70.6±12.5% 78.8±0.4% 71.7±0.3% 65.4±3.9% 71.1±0.3%
Average bits
32 4 1.70 32 4 2.03 32 4 1.87 32 4 2.54 32 4 2.12 32 4 2.65
Compression Ratio Speedup
1x — 8x 1x 18.6x 2.00x 1x — 8x 1x 15.4x 1.49x 1x — 8x 1x 17.0x 1.91x 1x — 8x 1x 12.6x 1.37x 1x — 8x 1x 15.1x 1.38x 1x — 8x 1x 12.1x 1.28x
       and add these gradients together. After the model has been trained, we obtain the quantization parameters (s, b). Since qmax can be calculated and sorted in advance, searching the nearest qmax can be implemented by binary searching. Usually, we set m = 1000 for all graph-level tasks in our paper and the overhead introduced to inference time is negligible.
4 EXPERIMENTS
4.1 EXPERIMENTAL SETTINGS
In this section, we evaluate our method on three typical GNN models, i.e., GCN, GIN, and GAT. And we compare our method with the FP32 GNN model and DQ-INT4 (Tailor et al., 2020) on eight datasets, including four node-level semi-learning tasks (Cora, CiteSeer, PubMed, ogbn-arxiv) (Hu et al., 2020; Yang et al., 2016) and four graph-level tasks (REDDIT-BINARY, MNIST, CI- FAR10, ZINC) (Yanardag & Vishwanathan, 2015; Dwivedi et al., 2020), to demonstrate the gen- erality and robustness of our method. Among these datasets, ZINC is a dataset for regression tasks, which uses regression loss as the metric of the model performance, while others are all for classifi- cation tasks.
For a fair comparison, we set the quantization bitwidth of W for all GNNs to 4bits as DQ-INT4. We count the average bitwidths for nodes features in all layers of the overall model and list them in our
6

Published as a conference paper at ICLR 2023
 Table 2: The results comparison on graph-level tasks.
  Dataset
MNIST
CIFAR10
Model
GCN(FP32) GCN(DQ) GCN(ours) GIN(FP32) GIN(DQ) GIN(ours) GCN(FP32) GCN(DQ) GCN(ours) GAT(FP32) GAT(DQ) GAT(ours) GCN(FP32)
Accuracy (Loss↓) 90.1±0.2%
84.4±1.3%
89.9±0.8%
96.4±0.4% 95.5±0.4% 95.7±0.2% 55.9±0.4% 51.1±0.7% 52.5±0.8% 65.4±0.4% 56.5±0.6% 64.7±2.8% 0.450±0.008 0.536±0.011 0.492±0.056 92.2±2.3% 81.3±4.4% 90.8±1.8%
Average bits
32 4 3.50 32 4 3.75 32 4 3.32 32 4 3.73 32 4 3.68 32 4 3.50
Compression ratio Speedup
1x — 8x 1x 9.12x 1.17x 1x — 8x 1x 8.52x 1.07x 1x — 8x 1x 9.62x 1.25x 1x — 8x 1x 8.57x 1.12x 1x — 8x 1x 8.68x 1.08x 1x — 8x 1x 9.14x 1.16x
     ZINC GCN(DQ) GCN(ours)
 REDDIT- BINARY
GIN(FP32) GIN(DQ) GIN(ours)
 results, denoted by “Average bits”. Since today’s CPUs and GPUs can not support mixed-precision operations well, we implement a precision-scalable hardware accelerator to perform the overall in- ference process for GNN. The accelerator employs massive bit-serial multipliers Judd et al. (2016), therefore, the latency of the integer multiplications is determined by the bitwidth of the node fea- tures. To evaluate the performance gains of our method over DQ-INT4, we develop a cycle-accurate simulator for our accelerator. More details about accelerator architecture are shown in Appendix A.7.5. Moreover, we show the compression ratio of quantized GNNs compared to the FP32 models in terms of overall memory size. For simplicity, we use GNN(DQ) to represent the GNNs quantized by DQ-INT4 and GNN-dataset to represent the task in which we run the experiment, e.g., GCN- Cora represents the GCN model trained on Cora. Detailed information about datasets and settings is in Appendix A.5 and Appendix A.6.
4.2 NODE-LEVEL TASKS
Table 1 shows the experimental results on three GNN architectures trained on four node-level datasets. Compared with DQ-INT4, our method can achieve significantly better accuracy on each task, even with a higher compression ratio, improving the inference performance with 1.28x to 2.00x speedups. On almost all node-level tasks, our proposed A2 Q has negligible accuracy drop compared to the FP32 baselines while achieving 12.1x-18.6x compression ratio. Since both GIN and GAT in- volve more complex computations, such as the calculation of attention coefficients in GAT, it is more challenging to quantize those models, and DQ performs poorly on these two models. How- ever, our method can overcome this problem and maintain comparable accuracy compared with the FP32 models. Our method can outperform the DQ-INT4 by 11.4% on the GAT-Cora task with a smaller bitwidth (2.03 v.s. 4). Even on ogbn-arxiv, which has a large number of nodes, A2Q can achieve a 12.1x compression ratio compared with FP32 baseline with comparable accuracy, which demonstrates the robustness of our method. Moreover, to demonstrate the generality of our method, we also evaluate our method on heterogeneous graphs and the inductive learning tasks and compare with more related works in Appendix A.7.1.
4.3 GRAPH-LEVEL TASKS
Table 2 presents the comparison results on the graph-level tasks. Our method can obtain better results on all tasks than DQ-INT4 with higher compression and a considerable speedup. Especially on the GIN-REDDIT-BINARY task, our method outperforms DQ-INT4 by 9.5% while achieving a 1.16x
7

Published as a conference paper at ICLR 2023

 

 
   
(a) GCN-CiteSeer (b) GIN-CiteSeer (c) GAT-CiteSeer (d) The first layer (e) The second layer
Figure 4: The relationship between quantized bitwidth and average in-degrees of nodes. (a), (b) and (c) represent the results of three GNN models trained on CiteSeer. (d) and (e) are results about the first and the second layer of an MLP, which is the update function of GIN trained on REDDIT- BINARY. The green bars represent the average in-degrees for the certain bitwidth used by nodes and the orange polylines represent the number of the nodes that use this certain bitwidth.
speedup. Even for graph datasets with similar in-degrees, such as MNIST and CIFAR10, our method also learns the appropriate bitwidths for higher compression ratio and better accuracy. Although on GIN-MINST task, the improvement of our method is relatively small due to the similarity of the in- degrees between different nodes, our method can achieve comparable accuracy with smaller bitwidth (3.75 v.s. 4).
4.4 ANALYSIS
To understand why our approach works, we analyze the relationship between the learned bitwidths and the topology of the graph. Figure 4(a) and 4(b) reveal that the bitwidth learned by A2Q is strongly related to the topology of graph data in the node-level tasks. As the bitwidth increases, the average in-degrees of nodes become larger. In other words, A2Q method tends to learn higher bitwidth for nodes with higher in-degrees. However, in GAT, as shown in Figure 4(c), the learned bits are irregular. This is because the features aggregated in GAT are topology-free. However, our method can still learn appropriate quantization bitwidths for different nodes, which improves accuracy while reducing memory usage. In addition, Figure 4 also shows the node distribution for different bitwidths and the result is consistent with power-law distribution. Since nodes in graph data mainly have low in-degrees, most of the nodes are quantized to low bitwidth (≤ 4), compressing the GNNs as much as possible. And there are also some high in-degree nodes quantized to high bitwidth, which can help to maintain the accuracy of the GNN models. As a result, the average bitwidth of the entire graph features is low, and the accuracy degradation is negligible.
For the graph-level tasks in which the number of nodes varies, our method is also aggregation- aware. We select a layer of GIN trained on REDDIT-BINARY and analyze the relationship between bitwidth and average in-degrees of nodes using the corresponding bitwidth to quantize in Figure 4(d) and 4(e). It can be seen that the bitwidth learned for nodes features input to the second layer of MLP, which is the update function in GIN for graph-level tasks, does not present a correlation with the topology of graph. We analyze the reason and find that the node features before the second layer is the result mapped by the first layer of MLP and is activated by the activation function, e.g., ReLU, which results in the node features losing the topology information. We present more experiment results in Appendix A.7. to demonstrate that our method is generally applicable.
5 ABLATION STUDY
The advantage of learning-based mixed-precision quantization: In Figure 5, we compare our A2 Q with the manual mixed-precision method, which manually assigns high-bit to those nodes with high in-degrees and low-bit to those nodes with low in-degrees. In the figure, the postfix “learn” denotes that using A2Q method, “manual” denotes that we assign bits to nodes and the model only learns the stepsize, and “mixed-precision” denotes that the model uses the same quantization method as DQ-INT4 but assigning different bitwidths to nodes. For the “mixed-precision”, we assign 5bits to those nodes with 50% top in-degrees and assign 3bits to others. The implications are two-fold. First, compared with the DQ-INT4, which uses the same quantization bitwidth, the mixed-precision
   


 
  %W %W
8
   



      
 %W




     


 
    
   
  
 %W
    
 
  
     
 
%W
 
1XPEHURIQRGHV
  
    
 
 


 
$YHUDHQGHUHH

Published as a conference paper at ICLR 2023
 Table 3: Ablation Study.
       
 
  $YHUDH%WV
 
 
7HVW$FFXUDF
               Model
Config
Accuracy
Average bits
     no-lr no-lr-b GIN-Cora no-lr-s
33.7±4.1% 4 75.6±0.2% 4 56.1±4.9% 3.85 77.8±1.6% 2.37 71.1±0.7% 32 56.8±6.7% 3 70.6±1.1% 1.87


   GCN- CiteSeer
lr-all
FP32 Global Local
Figure 5: The comparison between learning bitwidth and assign manually.
 method obtains 1.1% gains on GCN-Cora tasks demonstrating that the mixed-precision method is more effective. Second, the results of the learning method outperform the manual method on all tasks. Especially for the models with a high compression ratio, on GIN-CiteSeer task, learning method can achieve 21.5% higher accuracy. This demonstrates that our learning method can perform better than the assignment method according to prior knowledge for mixed-precision quantization of GNNs.
The power of learning the quantization parameters: Ablations of two quantization parameters (s, b) on the GIN-Cora task are reported in the first row of Table 3. The “no-lr” denotes that do not use learning method, “no-lr-b” denotes that only learn the step size s , “no-lr-s” denotes that only learn the bitwidths b, and “lr-all” denotes that learn the bitwidth and step size simultaneously. We can see that learning the step size can significantly increase the accuracy and even the “no-lr-bit” model can outperform the DQ-INT4 at the same compression ratio. When learning the bitwidth and step size simultaneously, the model can achieve higher accuracy with a higher compression ratio. This is because our method learns lower bitwidths for most nodes with low in-degrees and higher bitwidths for a tiny fraction of nodes with high in-degrees, which can improve the compression ratio while achieving higher accuracy.
Local Gradient v.s. Global Gradient: To demonstrate the effectiveness of our Local Gradient method, we compare the models trained with and without it on the GCN-CiteSeer task in the last row of Table 3. The “Global” denotes that the model is trained with Eq. 3 and Eq. 4. The model trained with the local method outperforms the global method by 13.8% with a higher compression ratio. This is because the Local Gradient method can learn quantization parameters for all nodes, while only quantization parameters for a part of nodes can be updated with the Global Gradient method due to the extreme sparse connection in the graph on the node-level semi-supervised tasks.
The overhead of Nearest Neighbor Strategy: We evaluate the real inference time of the GIN model on the 2080ti GPU. On REDDIT-BINARY task, the model without the selection process requires 121.45ms, while it takes 122.60ms for the model with our Nearest Neighbor Strategy, which only introduces 0.95% overhead. But with the help of the Nearest Neighbor Strategy, our model can obtain 19.3% accuracy gains for quantized GIN on REDDIT-BINARY.
6 CONCLUSION
This paper proposes A2Q, an aggregation-aware mixed-precision quantization method for GNNs, and introduces the Local Gradient and Nearest Neighbor Strategy to generalize A2Q to the node- level and graph-level tasks, respectively. Our method can learn the quantization parameters for different nodes by fully utilizing the property of GNNs and graph data. The model quantized by our A2Q can achieve up to a 18.6x compression ratio, and the accuracy degradation is negligible compared with the FP32 baseline. Compared with the prior SOTA, DQ-INT4, our method can significantly improve 11.4% accuracy with up to a 2.00x speedup on different tasks. Our work provides a general, robust and feasible solution to speed up the inference of GNNs.
9



&1&RUD
&1&RUD
1&WH6
1&WH6
1&WH6
1&WH6
&1&RUD
&1&RUD
 
HDUQ
PDQXD
HHUHDUQ
HHUPDQXD
HHU'417
HHUPHGSUHF
'417
PHGSUHFV
VRQ
RQ

Published as a conference paper at ICLR 2023
 REFERENCES
Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Su ̈sstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):2274–2282, 2012.
William Aiello, Fan Chung, and Linyuan Lu. A random graph model for power law graphs. Exper- imental mathematics, 10(1):53–66, 2001.
Mehdi Bahri, Gae ́tan Bahl, and Stefanos Zafeiriou. Binary graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9492–9501, 2021.
Rajeev Balasubramonian, Andrew B Kahng, Naveen Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. Cacti 7: New tools for interconnect exploration in innovative off-chip memories. ACM Transactions on Architecture and Code Optimization (TACO), 14(2):1–25, 2017.
Yoshua Bengio, Nicholas Le ́onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
John Brennan, Stephen Bonner, Amir Atapour-Abarghouei, Philip T Jackson, Boguslaw Obara, and Andrew Stephen McGough. Not half bad: Exploring half-precision in graph convolutional neural networks. In 2020 IEEE International Conference on Big Data (Big Data), pp. 2725–2734. IEEE, 2020.
Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016.
Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of neural networks via constrained optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5350–5359, 2021.
Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickerson, Furong Huang, and Tom Gold- stein. Vq-gnn: A universal framework to scale up graph neural networks using vector quantiza- tion. Advances in Neural Information Processing Systems, 34:6733–6746, 2021.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 293–302, 2019.
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems, 33:18518–18529, 2020.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen- dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.
Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and Yufei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized quantization. In 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), pp. 1044–1052. IEEE, 2020.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.
Rafael Go ́mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose ́ Miguel Herna ́ndez-Lobato, Benjam ́ın Sa ́nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Ala ́n Aspuru-Guzik. Automatic chemical design using a data-driven contin- uous representation of molecules. ACS central science, 4(2):268–276, 2018.
10

Published as a conference paper at ICLR 2023
 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.
Linyong Huang, Zhe Zhang, Zhaoyang Du, Shuangchen Li, Hongzhong Zheng, Yuan Xie, and Nianxiong Tan. Epquant: A graph neural network compression approach based on product quan- tization. Neurocomputing, 503:49–61, 2022.
Sambhav Jain, Albert Gural, Michael Wu, and Chris Dick. Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks. Proceedings of Machine Learning and Systems, 2:112–128, 2020.
Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. Multi-behavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval, pp. 659–668, 2020.
Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Meta-aggregator: Learning to aggregate for 1-bit graph neural networks. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 5301–5310, 2021.
Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor M Aamodt, and Andreas Moshovos. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM International Sym- posium on Microarchitecture (MICRO), pp. 1–12. IEEE, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net- works. arXiv preprint arXiv:1609.02907, 2016.
Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, and Alex Peysakhovich. Pytorch-biggraph: A large scale graph embedding system. Proceedings of Ma- chine Learning and Systems, 1:120–131, 2019.
Qian Lou, Feng Guo, Lantao Liu, Minje Kim, and Lei Jiang. Autoq: Automated kernel-wise neural network quantization. arXiv preprint arXiv:1902.05690, 2019.
Mike O’Connor. Highlights of the high-bandwidth memory (hbm) standard. In Memory forum workshop, volume 3, 2014.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks. Synthesis Lectures on Computer Architecture, 15(2):1–341, 2020.
Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. Degree-quant: Quantization- aware training for graph neural networks. arXiv preprint arXiv:2008.05000, 2020.
Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452, 2019.
Petar Velicˇkovic ́, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Hanchen Wang, Defu Lian, Ying Zhang, Lu Qin, Xiangjian He, Yiguang Lin, and Xuemin Lin. Binarized graph neural network. World Wide Web, 24(3):825–848, 2021a.
11

Published as a conference paper at ICLR 2023
 Junfu Wang, Yunhong Wang, Zhen Yang, Liang Yang, and Yuanfang Guo. Bi-gcn: Binary graph convolutional network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1561–1570, 2021b.
Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8612–8620, 2019.
Cong Xie, Ling Yan, Wu-Jun Li, and Zhihua Zhang. Distributed power-law graph computing: Theoretical and empirical analysis. Advances in neural information processing systems, 27, 2014.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1365–1374, 2015.
Hongxia Yang. Aligraph: A comprehensive graph neural network platform. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 3165– 3166, 2019.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph- saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.
Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik, and Pietro Lio. Learned low precision graph neural networks. arXiv preprint arXiv:2009.09232, 2020.
12

Published as a conference paper at ICLR 2023
 A APPENDIX
A.1 UNIFORM QUANTIZATION
In this section, we will give a detailed introduction to the content related to quantification.
A.1.1 QUANTIZATION PROCESS
For a vector x, the xq is a quantized representation. Given the quantization step size s, s ∈ R+, and
the quantization bitwidth b, b ∈ N+, then the uniform quantization is implemented as:
 |x| b−1 ⌊s +0.5⌋, |x|<s(2 −1)
2b−1 −1, |x|≥s(2b−1 −1)
The x at 32bits is mapped to the integer number set {−2b−1 + 1, ..., 0, ..., 2b−1 − 1} where the bitwidth is #b bits, and the quantized representation can be calculated as xq = s · x ̄. For inference, x ̄ can be used to compute matrix multiplication in the update phase or perform other computations in GNNs layers and the output of these computations then are rescaled by the corresponding s using a relatively lower cost scalar-vector multiplication. As an illustrative example, for vectors x ∈ R3×1 and y ∈ R3×1, the quantization parameters are both s = 0.1, b = 5, the process of inner product between these two vectors by integers is shown in Figure 6. When the values in a vector are all non-negative, we do not need to represent the sign bit in the fixed-point representation. Therefore, the value can use #b bits to quantize instead of using the first bit to represent the sign bit. Then the quantization range of uniform quantization is [−s(2b − 1), s(2b − 1)].
 x ̄ = s i g n ( x )
. ( 9 )
       0.31
    0.11 -0.21 1.29 × = -0.436
Preform by float-point representation
sign(0.11) ¥ Í 0.11 + 0.5 ̇ = 1 Í 0.1  ̇
1 -2 13 ×
= -48
× ! "= -0.48
    Í -0.21  ̇ sign(-0.21)¥Í +0.5 ̇=-2
= 0.1
!
= 0.1
"
   0.1¥(25-1 -1)=1.5
Quantization process
Î  ̊
-48
Rescale
-48
   Î0.1  ̊
Perform by integers representation
3
  0.58
6
  -0.27
-3
  Figure 6: An example of performing inner product by integers representation. A.1.2 GRADIENT IN BACKPROPAGATION
Due to the floor function used in the quantization process is not differentiable, the gradient of xq
with respect to x vanishes almost everywhere, which makes it impossible to train the model by the
backpropagation algorithm. Therefore, we use the straight-through estimator (Bengio et al., 2013) to
approximate the gradient through the floor function, i.e., ∂L = ∂L I|x|≤s(2b−1), where I|x|≤s(2b−1) ∂x ∂xq
is a indicator function, whose value is 1 when |x| ≤ s(2b − 1), and vice versa. In our paper, the quantification parameters (s, b) are learnable, the gradients of xq w.r.t. (s, b) used in Eq. 3 and Eq. 4 are:
      1 ( x − x )  ∂xq  s q ,
∂s=0
∂xq  sign(x)2b−1 −1,
∂b 2b−1 ln (2) s 13
|x|<s(2b−1−1)
. (10)
|x|≥s(2b−1 −1)
   
Published as a conference paper at ICLR 2023
 Table 4: The aggregation functions and update functions for GNNs used in this paper, di denotes the degree of node i, the ε denotes a learnable constant, and α represent attention coefficients.
 Model GCN GIN GAT
Aggregation function
Update function
(l) (l) (l) (l) xi =ReLU(W hi +b )
 (l) P
j∈N (i)∪{i}
1 (l−1) √ √ xj
hi =
di dj
    h(l) =(1+ε(l))x(l−1) + P x(l−1) x(l) =MLP(l)(h(l),W(l),b(l)) iijii
j∈N(i)
h(l) = P α(l)x(l−1) x(l) =W(l)hl +b(l) i i,jj i i
j∈N (i)∪{i}
  Table 5: The statistics for density of adjacency matrix and the labeled nodes in four node-level datasets.
 Cora CiteSeer
Density of A 0.144% 0.112% Labled nodes 5.17% 3.61%
A.2 MORE ABOUT GRAPH NEURAL NETWORKS
PubMed ogbn-arxiv
0.028% 0.008% 0.30% 53.70%
  In this section, we first give detailed information about the MPNN framework (Gilmer et al., 2017), and then provide a detailed examination of the three GNNs used in our papers.
A graph G = (V,E) consist of nodes V = {1,...,N} and edges E ⊆ V × V has node features X ∈ RN×F and optionally H-dimensional edge features E ∈ RE×H. The MPNN framework can
beformulatedbyx(l) =γ(l)(x(l−1),  φ(l)(x(l−1),x(l−1),e(l−1))),whereφisadifferentiable ii ijij
j∈N(i)
kernel function,  is the aggregation function which is permutation-invariant, and the γ is a learnable
update function, xi is the features of node i and eij is the features of edge between node i and j, N(i) = {j : (i,j) ∈ E}, and l represents the l-th layer of the GNNs.
In this paper, we focus on three typical GNN models whose forwardpass all can be represented by the MPNN framework, Graph Convolution Network (GCN) (Kipf & Welling, 2016), Graph Iso- morphism Network (GIN) (Xu et al., 2018), and Graph Attention Network (GAT) (Velicˇkovic ́ et al., 2017). the detailed information is shown in Table 4.
A.3 PROOFS OF THEORETICAL RESULTS
This section provides formal proof of the theoretical results of our paper.
A.3.1 NOTATIONS
Here, we define the notations utilized in our proof. A = {0, 1}N ×N is the adjacency matrix that indicates whether there is an edge between each pair of nodes, e.g., if there is an edge between node iandnodej,thenaij =1,otherwise,aij =0.Then,A ̃=A+Iistheadjacencymatrixforagraph
14

Published as a conference paper at ICLR 2023
 that is added to the self-loops. The degree matrix D = diag(d1, d2, ..., dn), where di = Pj aij and the degree matrix for the graph having self-loops is D ̃ = (d ̃ , d ̃ , ..., d ̃ ), where d ̃ = P a ̃ .
A.3.2 PROOFS
Proof 1. The gradients of the loss function with respect to the node features in semi-supervised tasks are most zero.
Without loss of generality, we use the GCN model as an example. From the Table 4, the graph convolution operation can be described as
X(l+1) = σ(AˆX(l)W (l)),
ˆ  ̃−1  ̃  ̃−1 (l)
∈ R
(l − 1)-th layer in GCN. σ is the non-linear activation function, e.g., ReLU. Note that the Aˆ is an
where A = D 2 AD 2 , is the normalized adjacency matrix, W
weight matrix in the l-th layer of GCN. X(l) is the input of the l-th layer and the output of the
12n ijij
Fin×Fout
(11) is a learnable
  extreme sparse matrix for node-level datasets in our paper.
In our training process of the model, we use nll loss as our task loss function L. Only the nodes in the train set T have labels. For the last layer of GCN, we get the node features to be classified by H(l+1) = softmax(X(l+1)). Then the gradient of L with respect to X(l+1) is
 1 ∂L ∂H(l+1) N×Fout G =∇X(l+1)L=∂H(l+1) ·∂X(l+1) =[lij]∈R
, (12) where only the G1i,:, i ∈ T is not zero, otherwise, G1i,: = 0. Then, the gradient of the loss function
  with respect to X(l) is
G2 = ∇X(l) L = AˆT (∇X(l+1) L ⊙ σ′(AˆX(l)W (l)))(W (l))T . (13)
For node j do not have an edge with the node i, i ∈ T, G2j,: = 0. Table 5 lists the density of the adjacency matrix A and the percentage of the labeled nodes in four node-level datasets. Because the sparsity property of adjacency matrix and the nodes with trained labels only account for a tiny fraction of the graph, the gradients from the loss function for most node features are zero.
Proof 2. The normalized adjacency matrix Aˆ is not needed to be quantized for the GCN model.
WetaketheprocessofXW →A(XW)asanillustrativeexample,whichrepresentsfirstcalculate the B = XW and then calculate AB. For the l-th layer of FP32 models, the first stage is Bl = XlWl,andthencalculatetheXl+1 =AˆBl,whereXl ∈RN×F1,Wl ∈RF1×F2 andA∈RN×N. The step-size for Bl, Xl and Wl is SBl , SXl and SWl , respectively. And they are all diagonal matrices. The integer representations are calculated as Bl = Bl q SBl , Xl = SXl Xl q and Wl = Wl q SWl . Note that for the node-level tasks, we can obtain the SBl , SXl and SWl in advance. And for the graph-level tasks, we can obtain them through one more element-wise multiplication whose overhead is negligible, as the comparison in Table 6. Then the first stage is:
Bl = Xl · Wl , (14) =(SXl ·Xlq)·(Wlq·SWl)
and there exists Bl = Bl q SBl . Therefore, the integers representation for the next stage can be calculated as:
      Bl q = BlS−1 Bl
=(SXl ·Xlq)·(Wlq·SWl)S−1 Bl
=(SXl ·Xlq)·(Wlq·(SWlS−1)) Bl
=(SXl ⊗(SWlS−1))⊙(Xl q ·Wl q) Bl
, (15)
       wherethe(SXl ⊗(SWlS−1))canbecalculatedoffline.Thenweobtainthefixed-pointrepresenta- Bl
tion Bl q for the next stage and do not introduce overhead.
The process of node degree normalization after the aggregation process can be represented as
X = σ(AˆB ), where Aˆ = D− 1 A ̃D− 1 is the normalized adjacency matrix, and σ is the l+1 l 2 2
   15

Published as a conference paper at ICLR 2023
  Figure 7: The pipeline of the quantization process on our accelerator.
−1 ̃
non-linear activation function. D 2 at the right side of A can be fused into the SXl and then
). And
.
Note that the elements in diagonal matrix SXl+1 are all positive because this matrix is made up of
−1 −1 ̃
step-size, which is always positive. Then we can obtain X(l+1) q = σ(SXl+1 D 2 ABl q ) , where
−1 −1  ̃ N×N  ̃
SXl+1 D 2 can be obtained before inference and A ∈ {0, 1} . The computation of ABl q only
−1 −1
has addition operations and the SXl+1 D 2 can be obtained before inference for node-level tasks or
introduce only once more element-wise multiplication to calculate for the graph-level tasks.
−1
The D 2 at the left side is fused into the element-wise multiplication performed by the next layer
−1
and the D 2 at the right side is fused into the element-wise multiplication performed by the current
layer and the element-wise multiplication is a necessary stage in the quantized model. Therefore, we can perform the node degree normalization using fixed point addition operation instead of quantizing the normalized adjacency matrix which may introduce more quantization error.
Proof 3. The quantization process can be fused with Batch Normalization operations.
When GNNs have Batch Normalization (BN) Layers, the calculation process is as follows (Note
that we have fused the mean and standard-deviation with the learned parameters in BN): Xl+1=BN(σ(AˆBlq)) , (17)
= σ ( Aˆ B l q ) Y + Z
where Y = diag(y1,y2,...,yF2) ∈ RF2×F2, Z = (z1,z2,...,zF2) ∈ RN×F2 and zi = (θi, θi, ..., θi)T ∈ RN among which yi and θi are the BN parameters for the i-th dimension fea- ture of the nodes features. And there exits that Xl+1 = SXl+1 Xl+1 q . Therefore,
Xl+1 q = S−1 Xl+1 Xl+1
=S−1 (σ(AˆBlq)Y+Z) . (18) Xl+1
=(S−1 ⊗Y)⊙(σ(AˆBlq))+S−1 Z Xl+1 Xl+1
Through Eq. 18, we can fuse the quantization of the next layer into the BN operation of the cur- rent layer, which will not introduce overhead because the BN layer itself requires floating point operations. Note that the float point operations are also element-wise.
 calculate Bl q as Eq. 15.
Then the features of the (l + 1)-th layer X can be obtained as X = σ(D− 1  ̃
 l+1 l+1 2 ABl q there exits Xl+1 = SXl+1 X(l+1) q . Therefore, the X(l+1) q can be obtained as:
    X =S−1X (l+1) q Xl+1
l+1
=S−1 σ(D−2A ̃Blq)
(16)
 1
 Xl+1
               16

Published as a conference paper at ICLR 2023
 Table 6: The comparison between fixed-point operations and float-point operations for some tasks using the Nearest Neighbor Strategy.
Task GIN-RE-IB GCN-MNIST GAT-CIFAR10 GCN-ZINC
   Fixed-point(M) 936.96 455.69 Float-point(M) 7.35 2.06 Ratio 0.78% 0.45%
1387.98 504.62 13.71 1.74 0.98% 0.34%
 A.4 THE OVERHEAD ANALYSIS OF NEAREST NEIGHBOR STRATEGY
Through our dedicated hardware and the optimized pipeline, we reduce the overhead introduced by the Nearest Neighbor Strategy (NNS) as much as possible. As the pipeline is shown in Figure 7, we fuse the (NNS) with the following operations. The fixed-point results produced by the previous stage are used to first multiply the corresponding step-size from the previous stage (an element- wise float point multiplication) and then execute the NNS process. After getting the step-size, these features are quantized immediately (an element-wise float point multiplication). Therefore, through this fusion way, we do not need the extra memory to store a copy of FP32 features.
In addition, the overhead of the NNS is from one more element-wise float point multiplication and the search process. We provide a comparison of the number of float-point operations and fixed-point operations for different graph-level tasks in Table 6, where ‘Fixed-point’ denotes the fixed-point op- eration, ‘Float-point’ denotes the float-point operation and the ‘Ratio’ denotes the percentage of the float-point operations in the overall process. The extra float-point operations introduced by NNS is only a tiny fraction of the fixed-point operations. On the other hand, through our optimized pipeline and the comparator array used in our accelerator the latency introduced by the search process of the NNS can be overlapped. Therefore, the overhead introduced by NNS is negligible.
A.5 DATASETS
We show the statistics for each dataset used in our work in Table 7. For datasets in node-level tasks, nodes correspond to documents and edges to citations between them. Node features are a bag-of-words representation of the document. The target is to classify each node in the graph cor- rectly. The Cora, CiteSeer and PubMed are from Yang et al. (2016). The ogbn-arxiv, ogbl-mag and ogbn-collab are from Hu et al. (2020). The Flickr is from Zeng et al. (2019). The Reddit is from Hamilton et al. (2017). In graph-level tasks, REDDIT-BINARY (Yanardag & Vishwanathan, 2015) is a balanced dataset where each graph corresponds to an online discussion thread and the nodes correspond to users. There would be an edge between two nodes if at least one of them responded to another’s comment. The task is then to identify whether a given graph belongs to a question/answer-based community or a discussion-based community The MNIST and CIFAR-10 datasets (Dwivedi et al., 2020) which are often used for image classification tasks are transformed into graphs in which every node is represented by their superpixel and location, and the edges are constructed by Achanta et al. (2012). The task is to classify the image using its graph representation. The ZINC Go ́mez-Bombarelli et al. (2018) dataset contains graphs representing molecules, where each node is an atom. The task is to regress the penalized logP (also called constrained solubility in some works) of a given graph. In Figure 8, we show the in-degree distribution for all the datasets we use in our paper.
A.6 EXPERIMENTAL SETUP
To make a fair comparison, we adopt the same GNN architectures as Tailor et al. (2020) on every task, and the FP32 baseline is also the same. For those tasks that Tailor et al. (2020) does not do, we adopt the same architecture as their FP32 version. For ogbn-arxiv and PubMed, we use the
17

Published as a conference paper at ICLR 2023
 Table 7: The statistics for each dataset used in this work.
  Task
Node-level
Graph-level
Name Graphs
Cora 1 CiteSeer 1 PubMed 1
ogbn-arxiv 1 ogbn-mag 1 ogbl-collab 1 Reddit 1 Flickr 1
REDDIT-BINARY 2000 MNIST 70000 CIFAR10 60000 ZINC 12000
Nodes
2708
 3327
 19717
169343
Edges
 10556
 9104
 88648
1166243
Features
Classes
 1939743 25582108
1433 7 3703 6 500 3
128 23 128 349 128 – 602 41 500 7
235868 232965 89250 ∼429.6 ∼71 ∼117.6 ∼23
 1285465
11606919
 899756
 ∼995.5 0 2 ∼565 3 10 ∼941.2 5 10 ∼49.8 28 —
 architectures and FP32 results reported by Hu et
We use standard splits for MNIST, CIFAR-10, and ZINC (Dwivedi et al., 2020). For Cora, CiteSeer and PubMed, we use the splits used by Yang et al. (2016). For REDDIT-BINARY, we use 10-fold cross-validation. Our data split way is also the same as DQ-INT4.
Figure 9 shows the architectures of the models used in our evaluations, including the layers, the number of hidden units, and whether to use a skip connection.
Our method is implemented using PyTorch Geometric (Fey & Lenssen, 2019). We quantize the same parts as the DQ-INT4 in all models except for the normalized adjacency matrix in the GCN model, which we have proven that the quantization of this matrix is not necessary in Appendix A.3.2, proof 2.. The values in the Cora and CiteSeer are all 0 or 1, therefore, we do not quantize the input features for the first layer of the GNNs trained on the two datasets as DQ. For all quantized GNNs, we train them by Adam optimizer. The learning rate and the learning rate schedule are consistent with their FP32 version. In our method, the quantization parameters (s, b) are also learnable, so we set the learning rate for them, including the b for features, s for features, and s for weights.
When initializing, the parameters of the models are initialized as their FP32 version, the quantization bits for all nodes and weight matrixes are initialized by 4bits, and the step sizes for node features and weights are initialized by s ∈ N (0.01, 0.01) except for the graph-level tasks on GAT, where we initialize the step size by s ∈ U(0,1). The N is normal distribution and the U is uniform distribution. And for GAT model trained on graph-level datasets, we just learn the quantization bits of the node features, while in the attention coefficients computation part, we use the exact 4bit to quantize. The batch size is 128 in all graph-level tasks. The results reported in our work for GNNs on Cora, CiteSeer and PubMed are averaged over 100 runs with different seeds, and the results for ogbn-arxiv, MNIST, CIFAR-10 and ZINC are averaged over ten runs. The results on REDDIT- BINARY are obtained by 10-fold cross-validation and the split seed is 12345, which is the same as DQ-INT4. All experiments in our paper ran on RTX 2080Ti GPU driven by Ubuntu 18.04. The version of the CUDA and Pytorch are 10.2 and 1.8.0, respectively.
A.6.1 EXPERIMENTAL SETUPS FOR THE ABLATION STUDY.
The advantage of learning-based mixed-precision quantization: During the experiment of com- paring the learning bitwidth and bit assignment, we ensure the average bits of node features of these two methods are comparable to verify the effectiveness of our A2Q method. As an example, if the average bit is 2.2bit when assigning the bit to nodes with different in-degrees, we will first sort the
al. (2020) and Kipf & Welling (2016) respectively.
18

Published as a conference paper at ICLR 2023
    







    QGHUHH
1RGH1XP
    







      QGHUHH
1RGH1XP
                         (a) Cora
(b) CiteSeer
  
     





    
QGHUHH
1RGH1XP
    









      QGHUHH
 1RGH1XP
                            (c) PubMed
(d) ogbn-arxiv
  







    QGHUHH
1RGH1XP
   
     





    QGHUHH
1RGH1XP
                                          (e) MNIST
(f) CIFAR10
 
      






    QGHUHH
   1RGH1XP
   





 QGHUHH
   1RGH1XP
                             (g) REDDIT-BINARY
Figure 8: The in-degree distribution for each dataset used in this work.
nodes by their in-degrees and then select the nodes with the top 20% in-degrees, and quantize those by 3bit, and for the remaining nodes, we use 2bit to quantize. In the model trained by the bit assign- ment method, the bit is not learnable, and other hyperparameters are all consistent with the model using the A2 Q method. For the “GCN-Cora-mixed-precision” and “GIN-CiteSeer-mixed-precision” tasks, we use 3bit and 5bit to quantize the GNNs while keeping the average bitwidth at 4bits. In particular, we assign 5bits to those nodes with 50% top in-degrees and assign 3bits to others.
19
(h) ZINC

Published as a conference paper at ICLR 2023
  Figure 9: The model architectures used in our evaluations, the head number for all GAT models on different tasks are 8.
Table 8: The results comparison on GCN-PubMed and GIN-ogbn-arxiv.
  Accuracy GCN(FP32) 78.9±0.7%
Average bits
32 4 1.90 32 4 3.82
Compression Ratio Speedup
1x — 8x 1x 16.8x 1.45x 1x — 8x 1x 8.4x 1.02x
 PubMed
ogbn-arxiv
GCN(DQ) GCN(ours) GIN(FP32) GIN(DQ) GIN(ours)
62.5±2.4%
77.5±0.1%
68.8±0.2% 57.6±2.2% 65.2±0.4%
  The power of learning the quantization parameters: For the “no-lr-bit”, we initialize the bitwidth as 4bits for all nodes features and just train the step size. For the “no-lr-step”, we initialize the step size as previously mentioned but do not train them. For the “no-lr”, we just initialize the bitwidth and the step size, but do not train them.
Local Gradient v.s. Global Gradient: All settings of the model trained by global gradient is consistent with the model trained by local gradient method.
The overhead of Nearest Neighbor Strategy: The model, without using the Nearest Neighbor Strategy, selects the quantization parameters according to their in-degrees. Every in-degree has a corresponding group of quantization parameters. Those nodes whose in-degrees are larger than 1000 will share the same group quantization parameters. In this way, The quantization parameters used by the nodes features can be determined as soon as the graph data is available, without the need for selection during the inference process, and then we can compare the overhead introduced by the selection process.
A.7 MORE EXPERIMENTS RESULTS
This section is a complementary part about experiments results to demonstrate that our A2Q quan- tization method is general and robust.
20

Published as a conference paper at ICLR 2023
 Table 9: The results comparison on
inductive learning tasks and more graphs.
  Task
GCN-mag
GCN-collab
GraphSage- REDDIT GraphSage- Flickr
Acc(%)
30.8±0.1(FP32) 32.7±0.4(Ours) 44.8±1.1(FP32) 44.9±1.5(Ours) 95.2±0.1(FP32) 95.3±0.1(Ours) 50.9±1.0(FP32) 50.0±0.5%(Ours)
Average bits
32 2.7 32 2.5 32 3.9 32 3.8
Compression Ratio
1x 11.7x 1x 12.7x 1x 8.1x 1x 8.4x
     Table 10: Comparison with more quantization method.
  Task
GCN-Cora
GAT-CiteSeer
GraphSage-Cora
GraphSage- Flickr
Acc(%)
80.9±0.0(Half-pre) 80.9±0.6(Ours) 68.0±0.1(LPGNAS) 71.9±0.7(Ours) 74.3±0.1(LPGNAS) 74.5±0.2(Ours) 49.7±0.3(LPGNAS) 50.0±0.5(Ours)
Average Bits
16 1.7 8 1.9 12 2.7 8 3.8
Compression Ratio
1x 9.40x 1x 4.21x 1x 4.44x 1x 2.11x
         
  


 
%W

  

 
  

  $YHUDHQGHUHH
1XPEHURIQRGHV
    
  


  
 
%W

 


 
 
   $YHUDHQGHUHH
1XPEHURIQRGHV
    
 
 
 
 
 
 
%W
  
    



  $YHUDHQGHUHH
1XPEHURIQRGHV
   (a) GCN-Cora
(d) GCN-PubMed
(b) GIN-Cora
(c) GAT-Cora
(g) GIN-ogbn-arxiv
      




 
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 

 






 %W
$YHUDHQGHUHH
1XPEHURIQRGHV
      
     
 
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 
   


  
 
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
          

  


(e) GAT-PubMed (f) GCN-ogbn-arxiv
Figure 10: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize. (a), (b) and (c) Three GNN models trained on Cora. (d) and (e) GCN and GAT trained on PubMed, respectively. (f) and (g) GCN and GIN trained on ogbn-arxiv, respectively.
21
 
     


 
     

Published as a conference paper at ICLR 2023
 Table 11: The effect of #m on the accuracy of quantized model, using GIN trained on REDDIT- BINARY as an example. The average bitwidth is 4bits.
GIN(FP32): 92.2± 2.3% GIN(DQ): 81.3± 4.4% m 100 400 800 1000 1500
Accuracy 88.7±3.5% 90.6±3.8% 92.0±2.2% 92.5±1.8% 92.6±1.9%
A.7.1 NODE-LEVEL TASKS
In Table 8, we show more task results on PubMed and ogbn-arxiv. On the GAT-ogbn-arxiv task, our GPU raised the Out Of Memory error, so we do not report the results on the GAT-ogbn-arxiv task. The model quantized by our A2Q method is also significantly better than DQ-INT4, which shows that our A2 Q is general. We do not compare with DQ-INT8 because our results are comparable with the FP32 baseline with a much larger compression ratio than DQ-INT8.
We also show the relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on more tasks in Figure 10. We present the results of the final layer of GNNs. The results show that the bitwidth learned by our A2Q method is also aggregation-aware, which means that our method is robust.
We also evaluate the inductive model, GraphSage, on some other node-level tasks to demonstrate the generality of our method on inductive learning tasks. Due to the sampling operation in the GraphSage model, the subgraph input to the model varies, we apply our nearest neighbor strategy to these tasks, i.e., GraphSage-Flickr and GraphSage-Reddit. In addition, we evaluate our method on more datasets, such as the ogbn-mag and ogbl-collab. ogbn-mag is a heterogeneous graph and the ogbl-collab is used for the link prediction tasks.
The results of our experiments are presented in Table 9, where we can see that our approach still works well and even brings some generalization performance improvement while significantly com- pressing the model size. This also demonstrates that our Neighbor Nearest Strategy generalizes well on inductive models for node-level tasks.
We also compare with more quantization methods on GNNs. Zhao et al. (2020) uses the Network Architecture Search (NAS) to search for the best quantization strategy for different components in the GNNs. Brennan et al. (2020) explore the use of half-precision (i.e., FP16) in the forward and backward passes of GNNs. Table 10 presents the comparison results on various tasks with these two methods. ‘Half-pre’ denotes the method in Brennan et al. (2020), and ‘LPGNAS’ denotes the method in Zhao et al. (2020). The results demonstrate that our method achieves better accuracy with a smaller quantization bitwidth on all tasks.
A.7.2 GRAPH-LEVEL TASKS
We propose the Nearest Neighbor Strategy to quantize the node features in graph-level tasks, in which the number of nodes input to models is various. In our Nearest Neighbor Strategy, #m groups quantization parameters (s, b) should be initialized, and we explore the effect of the value of m on the performance of the quantized model in Table 11 using the GIN trained on REDDIT-BINARY dataset. We can observe that when the value of m is smaller than 800, the accuracy increases as the value of m increases. When the value of m is higher than 800, the performances of the models with different m are similar. However, the models with a larger m are more stable.
Moreover, the selection of m may be related to the number of nodes input to the model. According to our experiments, we finally select m as 1000 for all graph-level tasks.
Table 12 lists the comparison results on GIN-ZINC and GAT-ZINC. On the regression tasks, our method is also significantly better than DQ-INT4. Notably, we do not learn different bitwidths for the nodes in ZINC datasets due to the similar topology structure between nodes.
   22

Published as a conference paper at ICLR 2023
 Table 12: The results comparison on GIN-ZINC and GAT-ZINC.
  Modle
ZINC
Dataset Loss↓ GAT(FP32) 0.455±0.006
GAT(DQ) 0.520±0.021 GAT(ours) 0.495±0.006 GIN(FP32) 0.334±0.024 GIN(DQ) 0.431±0.012 GIN(ours) 0.380±0.022
Average bits
Compression Ratio
 32 4 4 32 4 4
1x
8x
8x
1x
8x
8x
      
  

  
 %W

$YHUDHQGHUHH
1XPEHURIQRGHV
      
    
%W
      
 
$YHUDHQGHUHH
1XPEHURIQRGHV
 
    



 
 %W

$YHUDHQGHUHH
1XPEHURIQRGHV
 

  
   
  
%W

$YHUDHQGHUHH
1XPEHURIQRGHV
        
 (a) 1-st layer
(b) 2-nd layer
(c) 3-rd layer
(d) 4-th layer

Figure 11: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on CIFAR10.
(a) 1-st layer (b) 2-nd layer (c) 3-rd layer (d) 4-th layer
Figure 12: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on CIFAR10.
We also show the relationship between bit and average in-degree of nodes using the correspond- ing bit to quantize for more graph-level tasks in different layers immediately after the aggregation phase in Figure 11-Figure 16. The quantization bitwidths learned for graph-level tasks are also aggregation-aware. Because the difference of the in-degrees between different nodes is little in the MNIST and CIFAR10 dataset resulting in the aggregated features are similar between different nodes, the relationship between learned bitwidths and the in-degrees is irregular in some layers, e.g., the 2-nd layer in GCN trained on MNIST.
23

      

       
  

  
 %W

$YHUDHQGHUHH
1XPEHURIQRGHV
 
    



 
 %W

$YHUDHQGHUHH
1XPEHURIQRGHV
 



     
   
%W
  
$YHUDHQGHUHH
1XPEHURIQRGHV
   



   
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
        
      

  




   
Published as a conference paper at ICLR 2023
     
   
  
%W


$YHUDHQGHUHH
1XPEHURIQRGHV
    
   
  
%W


$YHUDHQGHUHH
1XPEHURIQRGHV
    
   
  
%W


$YHUDHQGHUHH
1XPEHURIQRGHV
    
   
  
%W


$YHUDHQGHUHH
1XPEHURIQRGHV
        

        
(a) 1-st layer (b) 2-nd layer (c) 3-rd layer (d) 4-th layer
Figure 13: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GAT trained on CIFAR10.
(a) 1-st layer (b) 2-nd layer (c) 3-rd layer (d) 4-th layer
Figure 14: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on MNIST.
(a) 1-st layer (b) 2-nd layer (c) 3-rd layer (d) 4-th layer
Figure 15: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on MNIST.
(a) 1-st layer (b) 2-nd layer (c) 3-rd layer (d) 4-th layer
Figure 16: The relationship between bit and average in-degrees of nodes using the corresponding bitwidth to quantize on different layers of GAT trained on MNIST.
24




   

  
 



 
 
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



 
 
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 





 
 
 %W
$YHUDHQGHUHH
1XPEHURIQRGHV
 








 %W
$YHUDHQGHUHH
1XPEHURIQRGHV
             


 

    


 
 
 



   
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



   
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



   
    
 %W
$YHUDHQGHUHH
1XPEHURIQRGHV
 

 
 
 
 
 %W
$YHUDHQGHUHH
1XPEHURIQRGHV
        


       




 
 



    
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



    
 
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



    
  
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
 



    
     
%W
$YHUDHQGHUHH
1XPEHURIQRGHV
               
  


   




  

Published as a conference paper at ICLR 2023
 Table 13: The impact of the depth of GNNs on quantization performance. Layers 3 4 5
   Task GCN-Cora GIN-Cora
Accu(%) Avarage Accu(%) Avarage Accu(%) Avarage
 FP32 Ours FP32 Ours
Bits 80.5±0.6 32
79.3±0.1
Bits Bits 32 75.8±3.2 32 3.54 75.0±1.2 3.61
80.2±0.6 2.94
49.4±15.8 32 37.1±13.1 32 — — 54.5±12.6 3.3 36.4±11.1 3.1 — —
78.2±0.9
  Table 14: The comparison between the model with and without skip connection on GCN-Cora task.
  Layers GCN-Cora
Without skip connection
With
skip connection FP32 Ours
 FP32
Accu(%) 80.5±0.6 Bits 32 Accu(%) 79.3±0.1 Bits 32
Accu(%) 75.8±3.2 Bits 32 Accu(%) 73.8±1.6
Ours
 3 4 5 6
80.2±0.6 82.5±0.5 82.2±0.7 2.94 32 2.37 78.2±0.9 81.9±0.7 81.5±0.3 3.54 32 2.63 75.0±1.2 81.1±1.1 80.6±0.6 3.61 32 2.72 73.1±1.9 80.1±0.8 80.4±0.7
   Bits 32 4.62 32 2.98
     
 
 


   




    $UPHDQ DHU
%WGW
        
  
  $UPHDQ
      
DHU
   $YEWVWVS $YEWVRVS 4(UURUWVS 4(UURURVS



 


 




  
            %WGW
4XDQWDWRQHUURU
      Figure 17: The average bitwidth for 2nd-5th layer in five layers GCN.
A.7.3 MORE ABLATION STUDY
Figure 18: The average bitwidth and quan- tization error for 2nd-6th layer in six layers GCN.
The impact of the depth of GNNs on quantization performance: We explore how a different number of GNN layers impacts the quantization performance of GCN-Cora and GIN-CiteSeer. We explore the quantization performance on 3,4,5,6 layers GCN model and 3,4 layers GIN model (the GCN and GIN used in Table 1 are 2 layers). We did not explore the deeper GNN models because
25

Published as a conference paper at ICLR 2023
 Table 15: The comparison results on other aggregation functions.
   GIN sum GIN mean GIN max
Baseline(FP32)
77.6±1.1% 78.8±0.1% 78.6±1.6%
Ours Bit
77.8±1.6% 2.37 78.5±0.6% 2.37 78.6±0.5% 1.97
Compression Ratio
13.5x 13.5x 16.2x
            
@ @
@ @ QGHUHH
@
$YHUDH)HDWXUH$IWHU$UDWRQ
 6XP 0HDQ
 0D
                      Figure 19: The average aggregated nodes features in different in-degree groups for models with different aggregation functions.
the accuracy of the model decreases drastically as the number of model layers increases due to the over-smooth phenomenon in GNNs. As shown in Table 13, our method can also maintain the performance with a high compression ratio for the model with different layers compared with the FP32 model.
In addition, we observe that the learned quantization bitwidth increases with the number of layers. We analysis the average bitwidth used by 2nd to 5th layer for the five layers GCN model in Figure 17. Our method learns a higher bitwidth for the deeper layer. Due to the over-smooth phenomenon that exists in the deep layer, the embedding features of different nodes are similar in the deep layer. Therefore, we consider the deeper layer may need a higher quantization bitwidth to distinguish the embedding features of different nodes.
The impact of skip connection on quantization performance: The first column denoted by ‘With- out skip connection’ and the second column denoted by ‘With skip connection’ of 18 present the comparison results for different layers GCN on Cora datasets without skip connection and with skip connection, respectively. For the model with skip connection, our method is also effective. Our method learns a higher bitwidth for the deeper layer. Due to the over-smooth phenomenon that ex- ists in the deep layer, we consider that the deeper layer may need a higher quantization bitwidth to distinguish the embedding features of different nodes. and the higher learned quantization bitwidth for deeper layers also alleviate quantization error. And compared to the quantized model with a skip connection, the learned quantization bitwidths are higher for the quantized model without skip connection. Figure 18 presents that the quantization errors of the model with skip connection are always higher than the model without skip connection in every layer which means that the model without skip connection is more sensitive to the quantization error. Therefore, a higher quantization bitwidth is necessary for the model without skip connection to maintain the performance. We will add these analyses to the appendix in the revision.
Scalability for models that use other aggregation functions: To demonstrate that our method is also helpful to the GNNs using other aggregation functions rather than the sum function, we replace the aggregation function of the GIN model, which is based on the MPNN framework with mean and max functions, and we conduct the comparison experiment on the Cora dataset. As shown in Table
26

Published as a conference paper at ICLR 2023
 Table 16: The comparison reults with the binary quantization method on Cora and CiteSeer datasets.
  GCN(FP32) Bi-GCN GCN(ours) GIN(FP32)
Cora Bi-GIN GIN(ours)
Accuracy
81.5±0.7% 81.2±0.8% 81.4±0.7% 77.6±1.1% 33.7±6.6% 77.4±0.8% 83.1±0.4% 31.9±0% 82.6±0.5% 71.1±0.7% 70.7±2.4% 70.7±0.7% 66.1±0.9% 29.1±1.7% 65.6±1.5% 72.5±0.7% 20.6±2.6% 71.0±0.7%
Average bits
32 1 1.61 32 1 1.92 32 1 2.03 32 1 1.98 32 1 2.39 32 1 2.15
Compression ratio
1x 32x 19.9x 1x 32x 16.7x 1x 32x 15.8x 1x 32x 16.2x 1x 32x 13.4x 1x 32x 14.9x
    CiteSeer
GAT(FP32) Bi-GAT GAT(ours) GCN(FP32) Bi-GCN GCN(ours) GIN(FP32) Bi-GIN GIN(ours) GAT(FP32) Bi-GAT GAT(ours)
  19, the accuracy degradation is negligible and the compression ratio is high , indicating that our quantization scheme also applies to the GNNs with mean or max aggregation function. We analyze the average features for different aggregation functions in different in-degrees group in Figure 19. The average features of the sum and max functions are highly dependent on in-degrees. The other insight is that the variance of the features is also highly dependent on in-degrees.
The analysis demonstrates the generality of our approach, which can capture differences between nodes introduced by topology information of graphs and compress the model size as much as possi- ble while maintaining the performance.
A.7.4 COMPARISON WITH BINARY QUANTIZATION METHOD
In this section, we show the advantages of our method over the binary quantization method for GNNs. We select the binary quantization method in Wang et al. (2021b) as our baseline. We just ran the experiments on the node-level because the binary quantization method only supports node-level tasks, which is one of the drawbacks of the binary quantization method in GNNs. We quantize the same part as Wang et al. (2021b) does for a fair comparison.
The comparison results are shown in Table 16. The binary quantization method performs well on GCN, where the aggregation and update phases are simple. However, on both models, GAT and GIN, the accuracy drops significantly compared with the FP32 baseline, which makes the deploy- ment unrealistic. However, our method is immune to this problem, although it has to use a higher average bit for node features which we believe is necessary for GAT and GIN. In summary, our method outperforms the binary quantization method in two ways:
1. Our method can quantize more complex GNN models and ensure the accuracy degradation is negligible compared with the FP32 baseline while achieving a high compression ratio of 13.4x- 19.9x.
2. Our method can be applied to graph-level tasks. However, the binary quantization method can not handle them.
27

Published as a conference paper at ICLR 2023
    Data Control signal
Decode
  Edge Buffer
Control Unit
   MAC MAC ... MAC
 MAC MAC...MACP
MAC MAC ... MAC MAC MAC ... MAC
DRAM
Weight Buffer
E
 Input Buffer
 Output Buffer
  2301 0 1 1
 +
   2211 0 1 1
Features 0 1 0 1
= =55 Weights 1 0 1 1
+
  +
   2101 0 1 1
    2011 0 1 1
      (a) (b)
Figure 20: (a) The overview of our accelerator architecture. (b) An example of the bit-serial calcu- lation and the architecture of the MAC.
A.7.5 ACCELERATOR ARCHITECTURE
In this section, we introduce the architecture of our hardware accelerator designed for GNN infer- ence. As presented in Section 3.1, we quantize each node feature to an appropriate precision and fix the weights to 4bits. To support mixed-precision computation, we adopt bit-serial multipliers at the core. Specifically, we follow the methodology in Judd et al. (2016) to only serialize the node features. This way, it takes m cycles to complete the multiplication between an m-bit node feature with a 4bit weight, as shown in Figure 20(b). The product involving 2n is implemented by left-shift, i.e., for 2n × a, we can shift a left by n bits to implement the product. To increase the computational throughput, we use 256 × 16 MACs which can process 256 16-dimensional features in parallel. As shown in Figure 20(a), the compute unit is composed of 256 Processing Engines (PEs), each containing a row of 16 MACs. The architecture of the MAC is shown in Figure 20(b).
The on-chip memory consists of an Edge Buffer, which stores the adjacency matrix of graphs, a Weight Buffer, which stores the weight of the GNNs, an Input Buffer, and an Output Buffer to store the input features and the output result, and the register of each MAC to store the partial sum. To reduce data movement in the memory hierarchy, the input buffer and output buffer work in a swapped fashion, as the output of the current layer is the input to the next layer. We set the memory size of Input Buffer, Output Buffer, Edge Buffer, and the Weight Buffer to 2MB, 2MB, 256KB, and 256KB, respectively. The overview of our architecture is shown in Figure 20(a).
To calculate Bl = XlWl, 256 consecutive rows in Xl and a column of Wl are mapped onto the MAC array to compute 256 inner products in each phase. To achieve this, a column of W l is broadcast and shared among PEs. The results of the inner products are written to the output buffer, which can be reused to reduce the off-chip DRAM access. The calculation of Xl+1 = ABl is also in a inner-product manner. In this scenario, A is a sparse matrix. We therefore represent A in the Compressed Sparse Row (CSR) format, where full zero rows or elements of A are eliminated. During inference, consecutive compressed rows of A and a column of Bl are mapped onto the MAC array in each phase. We also sort the nodes in descending order according to their in-degrees, and the nodes with similar in-degrees are processed in parallel simultaneously to alleviate the load imbalance problem when performing the aggregation operations.
A.7.6 ENERGY EFFICIENCY ANALYSIS
Our method can save energy cost significantly from the following two aspects:
1. By compressing the model size as much as possible, e.g., 18.6x compression ratio on GCN-Cora as shown in Table 1, our method can significantly reduce the memory footprints. Figure 21 presents the energy table for the 45nm technology node. It shows that memory access consumes further more energy than arithmetic operations. Therefore, the memory footprints domains the energy cost, and then compressing the model can save much energy cost.
2. Through our quantization method and the accelerator, the model can perform inference using the fixed-point operations instead of float-point operations, which are much more energy-consuming
28
0101
1 0 1 1
& + reg
<<

Published as a conference paper at ICLR 2023
        
 
&1&RUD
1&WH6HHU
$73XE0HG
&10167 $7&)$5 15(% HR0HDQ 7DVV
  38
 $4
 ×
 ×
×
×
× ×

×
×
×
× × × × ×
  (QHU(IIFHQF)23V
       Figure 21: The energy table for 45nm Figure 22: The energy efficiency compared technology node(Han et al., 2016; Sze et al., with 2080Ti GPU on various tasks.
2020).
than fixed-point operations. As shown in Figure 21, the 32bit float MULT consumes 18.5x energy compared to the 8bit int MULT. Therefore, our method’s energy consumption is much lower than the FP32 model.
To illustrate the advantage of our approach in terms of energy efficiency, we compare our accelerator with the 2080Ti GPU on various tasks. To estimate the energy efficiency of GPU, we use the nvidia- smi to obtain the power of GPU when performing the inference and measure the inference time by time function provided by Python. Then we can get the energy cost of GPU. We also model the energy cost of our method on the accelerator. We use High Bandwidth Memory (HBM) as our off- chip storage. Then we count the number of integer operations, and floating point operations, and the number of accesses to SRAM and HBM when performing the inference process of the quantized models on our accelerator. Based on the data in Table 21, we estimate the energy consumed by fixed- point operations and floating-point operations. The static and dynamic power of SRAM is estimated using CACTI 7.0(Balasubramonian et al., 2017). The energy of HBM 1.0 is estimated with 7 pJ/bit as in (O’Connor, 2014). Figure 22 presents these results, which shows that the the energy efficiency of our method is significantly better than GPU.
A.8 COMPLEXITY ANALYSIS
In this section, we provide the analysis of the complexity of our proposed A2Q method, including the computational complexity and space complexity.
Space Complexity: When analyzing the space complexity, we use the data size of the node features as an approximation of the entire loaded data, including weights and features, because the node features account for more than 90% of the overall memory consumption for a GNN model. For a GNN has L layers, we assume that the input data to the first layer is X ∈ RN ×F0 , and the dimension of the hidden features is F1. Then the dimension of the input to the 2-(L-1) layer is N × F1. After quantizing the model, the average bits of the feature maps are bm. The memory size includes two parts: 1. the nodes features bm[NF0 + (L − 1)NF1]. 2. the quantization step size (a step size is a float-point number which is 32bit) for each node 32NL. Therefore, the space complexity of the overall GNN model is as follows:
M = bm[NF0 + (L − 1)NF1] + 32NL. (19) We can also obtain the ratio of the memory consumption of the step size in overall memory size:
r= 32NL . (20) bm[NF0 + (L − 1)NF1]
In the node-level tasks, the F0 is usually much larger than 32, e.g., 3703 in the CiteSeer dataset. Moreover, in the graph-level tasks, we usually set m = 1000, which is much smaller than the number of the input nodes to models, i.e., N. Therefore, although our method learns the quantization step size for each node the memory overhead introduced by the quantization step size is negligible.
Computational Complexity: The forward pass is divided into the aggregation and update phases according to the MPNN framework. The aggregation phase can be represented as Hl = AˆXl,
 29

Published as a conference paper at ICLR 2023
 and then the update phase calculates Xl+1 = HlWl. For Aˆ ∈ RN×N, Xl ∈ RN×F1, and Wl ∈ RF1×F2,thecomputationalcomplexityoftheFP32modelsisO(N2F1 +NF1F2),which are all the float-point operations. After quantizing the model, the float-point matrix multiplication can be replaced by integer multiplication, and the element-wise operation, which calculates the mul- tiplication between integers and float-point numbers according to the Eq. 2. Then the computational complexity is
C = OI(N2F1 + NF1F2) + OE(NF2), (21)
whereOI representsthecomplexityoftheintegersmultiplication,whosecostismuchlowerthanthe float-point operations, and the OE represents the complexity of the element-wise operations. Note that although we quantize each node features by different step size, the complexity of element-wise operation involving float-point is the same as the DQ-INT4 because the number of element-wise operations is equal to the number of the elements in a feature map, i.e., N × F2.
30

